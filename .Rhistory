}
betas_update
t(y - (x %*% initial_betas))%*%(x %*% initial_betas)
t(y - (x %*% betas_update))%*%(x %*% betas_update)
t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
print(gradient_beta)
betas_update[i] <- initial_betas[i] + gradient_beta
}
learning_rate <- 0.001
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
print(gradient_beta)
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
learning_rate <- 0.1
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
print(gradient_beta)
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.1
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
print(gradient_beta)
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
if (error_gradient < error) {
initial_betas <- betas_update
}
initial_betas
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
print(gradient_beta)
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
error_gradient
learning_rate <- 0.1
for (it in 1:50) {
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
print(gradient_beta)
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
if (error_gradient < error) {
initial_betas <- betas_update
}
}
learning_rate <- 0.1
for (it in 1:50) {
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
if (error_gradient < error) {
initial_betas <- betas_update
error <- error_gradient
}
}
error
error_gradient
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.1
for (it in 1:50) {
betas_update <- rep(NA, n_predictors)
for(i in 1:n_predictors){
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update)
if (error_gradient < error) {
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.1
for (it in 1:50) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.1
for (it in 1:50) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
initial_betas
View(x)
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.01
for (it in 1:50) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.01
for (it in 1:50) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.01
for (it in 1:100) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
for (it in 1:1000) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
learning_rate <- 0.1
for (it in 1:1000) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
initial_betas\
initial_betas
lm(Rings ~ Lenght + Diameter, data = abalone)
View(abalone)
View(abalone)
lm(Rings ~ Lenght, data = abalone)
abalone$Length
lm(Rings ~ abalone$Lenght, data = abalone)
lm('Rings' ~ 'Lenght', data = abalone)
lm(y ~ x)
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
lm(y ~ x)
x
x <- data.matrix(x)
lm(y ~ x)
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - x %*% initial_betas)
# Start gradient descent
learning_rate <- 0.1
for (it in 1:1000) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
betas_update
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
initial_betas
x[, 2]
cov(y, x[,2])
cov(y, x[,1])
cov(y, x[,2])
cov(y, x[,i])
cov(y, x[,1])
cov(y, x[,2])
initial_betas
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
initial_betas <- c(3.95235709880724, -0.45833542,  11.07510254,  10.7615367 ,   8.97544462, -19.78686686, -10.58182703,   8.7418058 ,  -0.05771567, -0.88259194)
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
# Start gradient descent
learning_rate <- 0.1
for (it in 1:1000) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- (-2/n_obs) * sum(x[,i] * (y - sum(initial_betas * x))) # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
sum(initial_betas * x)
initial_betas * x
initial_betas %*% x
initial_betas
initial_betas
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
initial_betas
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
sum(x * initial_betas)
x %*% initial_betas
sum(x %*% initial_betas)
x
(y - sum(x %*% initial_betas)
(y - sum(x %*% initial_betas))
y
(y - sum(x %*% initial_betas))
sum((y - sum(x %*% initial_betas)))
