gradient_beta <- -2 * sum(x[,i] * (y - sum(initial_betas * x)))/length(x)/n_obs # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
betas_update
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
initial_betas
x[, 2]
cov(y, x[,2])
cov(y, x[,1])
cov(y, x[,2])
cov(y, x[,i])
cov(y, x[,1])
cov(y, x[,2])
initial_betas
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
initial_betas <- c(3.95235709880724, -0.45833542,  11.07510254,  10.7615367 ,   8.97544462, -19.78686686, -10.58182703,   8.7418058 ,  -0.05771567, -0.88259194)
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
# Start gradient descent
learning_rate <- 0.1
for (it in 1:1000) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- (-2/n_obs) * sum(x[,i] * (y - sum(initial_betas * x))) # This formula comes from gradient descent
betas_update[i] <- initial_betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
initial_betas <- betas_update
error <- error_gradient
}
print(error)
}
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
sum(initial_betas * x)
initial_betas * x
initial_betas %*% x
initial_betas
initial_betas
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
initial_betas <- rep(NA, n_predictors) # vector to create the initial solution
initial_betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
initial_betas[i] <- cov(y, x[,i])/ var(x[,i])
}
initial_betas
t(y - (x %*% initial_betas))%*%(y - (x %*% initial_betas))
sum(x * initial_betas)
x %*% initial_betas
sum(x %*% initial_betas)
x
(y - sum(x %*% initial_betas)
(y - sum(x %*% initial_betas))
y
(y - sum(x %*% initial_betas))
sum((y - sum(x %*% initial_betas)))
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
## Here starts the package
n_obs <- length(y)
x <- cbind('1' = 1, x) # create a column with ones
x <- data.matrix(x) # transform the previous column to a matrix
n_predictors <- ncol(x) # obtained the number of predictors plus the constant
betas <- rep(NA, n_predictors) # vector to create the initial solution
betas[1] <- mean(y) # fill the first element with the mean of the response
# This for loop fill the initial solution
for (i in 2:n_predictors) {
betas[i] <- cov(y, x[,i])/ var(x[,i])
}
# Computing the error or loss function
error <- t(y - (x %*% betas))%*%(y - (x %*% betas))
# Start gradient descent
learning_rate <- 0.3
for (it in 1:30000) { # This for loop repeats the algorithm
betas_update <- rep(NA, n_predictors) # initialize the betas for update
for(i in 1:n_predictors){ # This for loop gets the gradient for each beta
gradient_beta <- (-2/n_obs) * sum(x[, i] * data.matrix(y - x %*% betas)) # This formula comes from gradient descent
betas_update[i] <- betas[i] - learning_rate * gradient_beta # Update the ith beta
}
error_gradient <-  t(y - (x %*% betas_update))%*%(y - x %*% betas_update) # Compute the loss function
if (error_gradient < error) { # Evaluate if there is improvement
betas <- betas_update
error <- error_gradient
}
} # Finish gradient descent
y_hat <- x %*% betas
y_mean <- mean(y)
SSE <- sum(data.matrix(y - x %*% betas)^2)
SST <- sum((y-y_mean)^2)
sigma_hat_sq = as.numeric(t(y-x%*%betas)%*%(y-x%*%betas)/(n_obs-n_predictors))
## R^2
Rsq = 1-(SSE/SST)
# C_p
Cp = SSE + 2*n_predictors*sigma_hat_sq
# confidence intervals
alpha <- 0.05
x_t_x = solve(t(x)%*%x)
D = rep(NA,n_predictors) # Initializing the diagonal
for (i in 1 : n_predictors){
D[i] = x_t_x[i,i] # Filling the diagonal up
}
ss = qnorm((1-alpha/2),mean=0,sd=1)*sqrt(sigma_hat_sq*D) # the right side of the interval
conf_int = cbind(betas-ss,betas+ss) # confidence intervals
DFM = n_predictors-1
DFE = n_obs - n_predictors
SSM = 0
for (i in 1 : n_obs){
SSM = SSM + (y_hat[i] - y_mean)^2
}
MSM = SSM/DFM
MSE = SSE/DFE
# F_star
F_star = MSM/MSE
# P-value
p_value = 1-pf(F_star, DFM, DFE)
error
betas
conf_int
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
lin_reg = function(explanatory,response,alpha = 0.05){
y = as.vector(response)
n = length(y)
X = cbind(rep(1,n),explanatory)
X = as.matrix(X)
## calculation of beta_hat
p = dim(X)[2]
y_mean = mean(y)
## initial beta
beta0 = rep(NA,p)
beta0[1] = y_mean
if (p>1){
for (i in 2 : p){
beta0[i] = cov(y,X[,i])
}
}
fnc = function(aa) t(y-X%*%aa)%*%(y-X%*%aa)
beta_hat = optim(beta0,fnc)$par
y_hat = X%*%beta_hat
sigma_hat_sq = as.numeric(t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)/(n-p))
SSE = 0
SST = 0
for (i in 1 : n){
SSE = SSE + (y[i] - y_hat[i])^2
SST = SST + (y[i]-y_mean)^2
}
## R^2
Rsq = 1-(SSE/SST)
# C_p
Cp = SSE + 2*p*sigma_hat_sq
# confidence intervals
X_t_X = solve(t(X)%*%X)
D = rep(NA,p)
for (i in 1 : p){
D[i] = X_t_X[i,i]
}
ss = qnorm((1-alpha/2),mean=0,sd=1)*sqrt(sigma_hat_sq*D)
conf_int = cbind(beta_hat-ss,beta_hat+ss)
DFM = p-1
DFE = n-p
SSM = 0
for (i in 1 : n){
SSM = SSM + (y_hat[i] - y_mean)^2
}
MSM = SSM/DFM
MSE = SSE/DFE
# F_star
F_star = MSM/MSE
# P-value
P = 1-pf(F_star,DFM,DFE)
return(list(beta_hat,conf_int,Rsq,Cp,F_star,P))
}
lin_reg(x, y)
optim?
?optim
lin_reg = function(explanatory,response,alpha = 0.05){
y = as.vector(response)
n = length(y)
X = cbind(rep(1,n),explanatory)
X = as.matrix(X)
## calculation of beta_hat
p = dim(X)[2]
y_mean = mean(y)
## initial beta
beta0 = rep(NA,p)
beta0[1] = y_mean
if (p>1){
for (i in 2 : p){
beta0[i] = cov(y,X[,i])/var(X[,i])
}
}
fnc = function(aa) t(y-X%*%aa)%*%(y-X%*%aa)
beta_hat = optim(beta0,fnc)$par
y_hat = X%*%beta_hat
sigma_hat_sq = as.numeric(t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)/(n-p))
SSE = 0
SST = 0
for (i in 1 : n){
SSE = SSE + (y[i] - y_hat[i])^2
SST = SST + (y[i]-y_mean)^2
}
## R^2
Rsq = 1-(SSE/SST)
# C_p
Cp = SSE + 2*p*sigma_hat_sq
# confidence intervals
X_t_X = solve(t(X)%*%X)
D = rep(NA,p)
for (i in 1 : p){
D[i] = X_t_X[i,i]
}
ss = qnorm((1-alpha/2),mean=0,sd=1)*sqrt(sigma_hat_sq*D)
conf_int = cbind(beta_hat-ss,beta_hat+ss)
DFM = p-1
DFE = n-p
SSM = 0
for (i in 1 : n){
SSM = SSM + (y_hat[i] - y_mean)^2
}
MSM = SSM/DFM
MSE = SSE/DFE
# F_star
F_star = MSM/MSE
# P-value
P = 1-pf(F_star,DFM,DFE)
return(list(beta_hat,conf_int,Rsq,Cp,F_star,P))
}
lin_reg(x, y)
t(y-x%*%x)%*%(y-x%*%x)
response <- lin_reg(x, y)
response[1]
t(y-x%*%response[1])%*%(y-x%*%response[1])
y
x
t(y-x%*%data.matrix(response[1]))%*%(y-x%*%data.matrix(response[1]))
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
lin_reg = function(explanatory,response,alpha = 0.05){
y = as.vector(response)
n = length(y)
X = cbind(rep(1,n),explanatory)
X = as.matrix(X)
## calculation of beta_hat
p = dim(X)[2]
y_mean = mean(y)
## initial beta
beta0 = rep(NA,p)
beta0[1] = y_mean
if (p>1){
for (i in 2 : p){
beta0[i] = cov(y,X[,i])/var(X[,i])
}
}
fnc = function(aa) t(y-X%*%aa)%*%(y-X%*%aa)
beta_hat = optim(beta0,fnc)$par
y_hat = X%*%beta_hat
sigma_hat_sq = as.numeric(t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)/(n-p))
SSE = 0
SST = 0
for (i in 1 : n){
SSE = SSE + (y[i] - y_hat[i])^2
SST = SST + (y[i]-y_mean)^2
}
## R^2
Rsq = 1-(SSE/SST)
# C_p
Cp = SSE + 2*p*sigma_hat_sq
# confidence intervals
X_t_X = solve(t(X)%*%X)
D = rep(NA,p)
for (i in 1 : p){
D[i] = X_t_X[i,i]
}
ss = qnorm((1-alpha/2),mean=0,sd=1)*sqrt(sigma_hat_sq*D)
conf_int = cbind(beta_hat-ss,beta_hat+ss)
DFM = p-1
DFE = n-p
SSM = 0
for (i in 1 : n){
SSM = SSM + (y_hat[i] - y_mean)^2
}
MSM = SSM/DFM
MSE = SSE/DFE
# F_star
F_star = MSM/MSE
# P-value
P = 1-pf(F_star,DFM,DFE)
print(SSE)
return(list(beta_hat,conf_int,Rsq,Cp,F_star,P))
}
response <- lin_reg(x, y)
SSE
# Import data
abalone <- read.csv(file = 'abalone.csv')
# Transform dummy variables
library(fastDummies)
abalone <- dummy_cols(abalone, select_columns = "Sex")
# Create response variable and set of predictors
y <- abalone$Rings
x <- abalone[, 2:11]
x <- subset(x, select = -Rings)
lin_reg = function(explanatory,response,alpha = 0.05){
y = as.vector(response)
n = length(y)
X = cbind(rep(1,n),explanatory)
X = as.matrix(X)
## calculation of beta_hat
p = dim(X)[2]
y_mean = mean(y)
## initial beta
beta0 = rep(NA,p)
beta0[1] = y_mean
if (p>1){
for (i in 2 : p){
beta0[i] = cov(y,X[,i])/var(X[,i])
}
}
fnc = function(aa) t(y-X%*%aa)%*%(y-X%*%aa)
beta_hat = optim(beta0,fnc,method = "CG")$par
y_hat = X%*%beta_hat
sigma_hat_sq = as.numeric(t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)/(n-p))
SSE = 0
SST = 0
for (i in 1 : n){
SSE = SSE + (y[i] - y_hat[i])^2
SST = SST + (y[i]-y_mean)^2
}
## R^2
Rsq = 1-(SSE/SST)
# C_p
Cp = SSE + 2*p*sigma_hat_sq
# confidence intervals
X_t_X = solve(t(X)%*%X)
D = rep(NA,p)
for (i in 1 : p){
D[i] = X_t_X[i,i]
}
ss = qnorm((1-alpha/2),mean=0,sd=1)*sqrt(sigma_hat_sq*D)
conf_int = cbind(beta_hat-ss,beta_hat+ss)
DFM = p-1
DFE = n-p
SSM = 0
for (i in 1 : n){
SSM = SSM + (y_hat[i] - y_mean)^2
}
MSM = SSM/DFM
MSE = SSE/DFE
# F_star
F_star = MSM/MSE
# P-value
P = 1-pf(F_star,DFM,DFE)
print(SSE)
return(list(beta_hat,conf_int,Rsq,Cp,F_star,P))
}
response <- lin_reg(x, y)
lin_reg = function(explanatory,response,alpha = 0.05){
y = as.vector(response)
n = length(y)
X = cbind(rep(1,n),explanatory)
X = as.matrix(X)
## calculation of beta_hat
p = dim(X)[2]
y_mean = mean(y)
## initial beta
beta0 = rep(NA,p)
beta0[1] = y_mean
if (p>1){
for (i in 2 : p){
beta0[i] = cov(y,X[,i])/var(X[,i])
}
}
fnc = function(aa) t(y-X%*%aa)%*%(y-X%*%aa)
beta_hat = optim(beta0,fnc,method = "L-BFGS-B")$par
y_hat = X%*%beta_hat
sigma_hat_sq = as.numeric(t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)/(n-p))
SSE = 0
SST = 0
for (i in 1 : n){
SSE = SSE + (y[i] - y_hat[i])^2
SST = SST + (y[i]-y_mean)^2
}
## R^2
Rsq = 1-(SSE/SST)
# C_p
Cp = SSE + 2*p*sigma_hat_sq
# confidence intervals
X_t_X = solve(t(X)%*%X)
D = rep(NA,p)
for (i in 1 : p){
D[i] = X_t_X[i,i]
}
ss = qnorm((1-alpha/2),mean=0,sd=1)*sqrt(sigma_hat_sq*D)
conf_int = cbind(beta_hat-ss,beta_hat+ss)
DFM = p-1
DFE = n-p
SSM = 0
for (i in 1 : n){
SSM = SSM + (y_hat[i] - y_mean)^2
}
MSM = SSM/DFM
MSE = SSE/DFE
# F_star
F_star = MSM/MSE
# P-value
P = 1-pf(F_star,DFM,DFE)
print(SSE)
return(list(beta_hat,conf_int,Rsq,Cp,F_star,P))
}
response <- lin_reg(x, y)
response[1]
